---
title: Services
description: Managed services to accelerate AI/ML development and deployment
---

# Services

TELUS AI Factory provides managed services designed to simplify and accelerate AI/ML development and deployment. These services offer predefined configurations that deliver ready-to-use environments and endpoints.

## Available Services

<Cards>
  <Card
    title="Notebooks"
    description="Interactive environments for data analysis and ML development"
    href="/docs/services/notebooks"
  />
  <Card
    title="vLLM Inference Services"
    description="Optimized serving for large language models"
    href="/docs/services/vllm"
  />
  <Card
    title="Ollama Inference Services"
    description="Local LLM deployment and management"
    href="/docs/services/ollama"
  />
  <Card
    title="NIM Services"
    description="NVIDIA Inference Microservices for optimized AI inference"
    href="/docs/services/nim"
  />
  <Card
    title="Marketplace Applications"
    description="Pre-configured ML solutions and tools"
    href="/docs/services/marketplace"
  />
</Cards>

## Service Categories

### Development Services

**Notebooks** provide interactive environments perfect for:
- Exploratory data analysis
- Model development and experimentation
- Collaborative research
- Educational purposes

### Inference Services

Deploy trained models as APIs with:
- **vLLM** - High-throughput LLM serving
- **Ollama** - Local LLM deployment
- **NIM** - NVIDIA-optimized inference

### Marketplace

Pre-built solutions and applications:
- Ready-to-deploy ML tools
- Integrated development environments
- Specialized ML frameworks
- Custom enterprise solutions

## Benefits of Managed Services

### Quick Deployment
- Pre-configured environments
- One-click deployment
- No complex setup required
- Start working in minutes

### Optimized Performance
- Purpose-built configurations
- Hardware acceleration
- Automatic scaling
- Performance tuning

### Simplified Management
- Automated updates
- Monitoring included
- Easy configuration
- Built-in security

### Cost Effective
- Pay for what you use
- No infrastructure overhead
- Optimized resource usage
- Predictable pricing

## Choosing the Right Service

### For Development and Research
Use **Notebooks** when you need:
- Interactive coding environment
- Data visualization tools
- Experiment tracking
- Collaborative features

### For Model Serving
Choose the right inference service:

- **vLLM**: Best for high-throughput LLM serving
- **Ollama**: Ideal for local development and testing
- **NIM**: Optimal for production NVIDIA-accelerated inference

### For Quick Solutions
Browse **Marketplace** for:
- Pre-built ML applications
- Industry-specific solutions
- Integrated tool suites
- Custom enterprise offerings

## Common Workflows

### Development to Production

1. **Develop** in Notebooks
   - Explore data
   - Build and train models
   - Test and validate

2. **Deploy** to Inference Service
   - Package your model
   - Choose inference service
   - Deploy as API endpoint

3. **Scale** with Kubernetes
   - Container your application
   - Deploy to cluster
   - Auto-scale based on demand

### Model Serving Pipeline

1. **Train** your model
2. **Export** in compatible format
3. **Create** inference endpoint
4. **Test** API responses
5. **Monitor** performance
6. **Optimize** as needed

## Next Steps

Explore each service to learn more about capabilities and how to get started:

- [Notebooks](/docs/services/notebooks) - Start developing interactively
- [vLLM](/docs/services/vllm) - Deploy LLM APIs
- [Ollama](/docs/services/ollama) - Run models locally
- [NIM](/docs/services/nim) - NVIDIA-optimized serving
- [Marketplace](/docs/services/marketplace) - Browse pre-built solutions
